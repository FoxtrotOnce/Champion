"""Defines PPO architecture, and implements PPO training for Spy Cards.

All PPO operations are defined and used here.
Model data is stored in a ppo_data.pkl file, and can be viewed in GraphPPO.py.
PPO training is as follows:
  - The actor predicts moves for a batch of episodes, and uses the trajectories
  to compute loss for the actor and critic.
  - Actor loss is computed by maximizing the surrogate objective, with entropy.
  - Critic loss is computed by minimizing the MSE of the one-step TD error.
  - Repeat for some amount of episodes.

Classes:
    Agent: Create an agent to perceive the game and make decisions.
    Slice: Custom TensorFlow layer for slicing input tensors.
    Model: Create a PPO policy to predict actions.

Methods:
    mask_actions: Mask invalid actions.
"""

import core
import PPO_config as config
import tensorflow as tf
import keras
import numpy as np
import pandas as pd
from math import log
import random
from timeit import timeit


np.set_printoptions(suppress=True, precision=4)


class Agent:
    """Create an agent to perceive the game and make decisions.

    The agent interacts with the models to estimate optimal outcomes.
    Actions are picked by following the policy generated by the actor.

    Attributes:
        actor (tf.keras.Model): The model that tries to find the optimal policy.
        critic (tf.keras.Model): The model that estimates state-values.
        buffer (list): Retains samples collected from each episode in the batch.

    Methods:
        act: Choose an action, following the actor's policy.
        fit: Compute the models' losses, and adjust the gradients.
        reset_lstm: Reset the models' LSTM states.
    """

    def __init__(self):
        """Create/load actor and critic models."""
        try:
            self.actor = keras.models.load_model('actor.keras')
            print('Successfully loaded the actor.')
        except ValueError:
            print('Could not load the actor.')
            self.actor = Model(is_actor=True)

        try:
            self.critic = keras.models.load_model('critic.keras')
            print('Successfully loaded the critic.')
        except ValueError:
            print('Could not load the critic.')
            self.critic = Model(is_actor=False)
        self.buffer = []

    def act(self, state: np.ndarray) -> tuple[int, float]:
        """Choose an action, following the actor's policy.

        Args:
            state (np.ndarray): The state to predict.

        Returns:
            tuple[int, float]:
                int: The chosen action.
                float: The log of the action's probability from the policy.
        """
        probs = self.actor.predict([state], verbose=0)[0]
        # Add 1e-8 to prevent all probs from being 0, which raises ValueError on random.choices
        probs += 1e-8
        mask_actions(env.player_hand, env.TP, probs)
        action = random.choices(range(32), probs, k=1)[0]
        log_prob = log(probs[action])

        return action, log_prob

    def fit(self) -> tuple[float, float, float]:
        """Compute the models' losses, and adjust the gradients.

        Actor loss is computed by maximizing the surrogate objective with entropy,
        and critic loss is computed by minimizing the MSE of the one-step TD error.

        Methods:
            _process_batched_states: Process the batched states using a model.

        Returns:
            tuple[float, float, float]:
                float: The actor's computed loss.
                float: The critic's computed loss.
                float: The average computed advantage (TD error).
        """
        def _process_batched_states(model: "Model") -> tf.Tensor:
            """Process the batched states using a model.

            Episodes will often have different lengths, so they cannot be
            processed together.
            This processor calls the model with each individual episode,
            and then flattens the results back together.

            Args:
                model (Model): The actor/critic to call upon.
            """
            estimates = []
            for batch_state in batch_states:
                self.reset_lstm()
                estimates.append(model(batch_state))
            estimates = tf.concat(estimates, axis=0)
            return estimates

        states, actions, rewards, old_log_probs, dones = zip(*self.buffer)

        states = tf.convert_to_tensor(states, tf.float32)
        actions = tf.convert_to_tensor(actions, tf.int32)
        rewards = tf.convert_to_tensor(rewards, tf.float32)
        old_log_probs = tf.convert_to_tensor(old_log_probs, tf.float32)
        dones = tf.convert_to_tensor(dones, tf.float32)

        # Batch states by episode, for individual processing.
        batch_states = [[]]
        for i, state in enumerate(states):
            batch_states[-1].append(state)
            if dones[i]:
                batch_states[-1] = tf.convert_to_tensor(batch_states[-1], tf.float32)
                batch_states.append([])
        # Remove extra empty batch
        batch_states.pop()

        with tf.GradientTape() as critic_tape:
            state_values = _process_batched_states(self.critic)
            # Next states are taken from existing states, so it is 1 short, and is padded with a zeroed array.
            next_sv = tf.concat([state_values[1:], tf.zeros((1, 1))], axis=0)

            targets = rewards + config.gamma * next_sv
            # Critic loss is gradient descent, leave as-is. Use MSE.
            critic_loss = tf.reduce_mean((targets - state_values) ** 2)
        critic_grads = critic_tape.gradient(critic_loss, self.critic.trainable_variables)
        self.critic.optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))

        with tf.GradientTape() as actor_tape:
            state_values = _process_batched_states(self.critic)
            next_sv = tf.concat([state_values[1:], tf.zeros((1, 1))], axis=0)

            targets = rewards + config.gamma * next_sv
            advantages = targets - state_values

            new_probs = _process_batched_states(self.actor)
            action_probs = tf.reduce_sum(new_probs * tf.one_hot(actions, 32), axis=-1)
            print(action_probs)
            new_log_probs = tf.math.log(action_probs)

            ratio = tf.exp(new_log_probs - old_log_probs)
            clipped_ratio = tf.clip_by_value(ratio, 1 - config.epsilon, 1 + config.epsilon)
            surrogate_obj = tf.minimum(ratio * advantages, clipped_ratio * advantages)
            surrogate_loss = tf.reduce_mean(surrogate_obj)

            entropy_bonus = -tf.reduce_sum(action_probs * new_log_probs)

            # Actor loss is gradient ascent, so loss must be multiplied by -1 for TF to ascend.
            actor_loss = -1 * (surrogate_loss + config.c2 * entropy_bonus)

        actor_grads = actor_tape.gradient(actor_loss, self.actor.trainable_variables)
        self.actor.optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))
        return actor_loss, critic_loss, tf.reduce_mean(advantages)

    def reset_lstm(self) -> None:
        """Reset the models' LSTM states.

        The LSTM layers are stateful, so this method is used for resetting
        states between episodes.
        """
        self.actor.lstm.reset_states()
        self.critic.lstm.reset_states()


def mask_actions(hand: list[core.Card], TP_limit: int, actions: np.ndarray) -> None:
    """Mask invalid actions.

    Modify actions in-place.
    Mask invalid actions to 0 that either use a card not present
    in the hand, or exceed the TP limit.

    Args:
        hand (list[core.Card]): A list of the cards in the player's hand.
            Used for checking available cards.
        TP_limit (int): The total TP cost of the move cannot exceed this limit.
        actions (np.ndarray): The actions to mask in-place.
    """
    for i, card_indices in enumerate(core.all_moves):
        # Mask move if it requires a card that isn't in the hand
        if card_indices and card_indices[-1] + 1 > len(hand):
            actions[i] = 0
            continue
        # Mask move if the cost exceeds the TP limit
        TP_used = 0
        for card_index in card_indices:
            TP_used += hand[card_index].TP
            if TP_used > TP_limit:
                actions[i] = 0
                break


@keras.saving.register_keras_serializable()
class Slice(tf.keras.layers.Layer):
    """Custom TensorFlow layer for slicing input tensors.

    Meant for slicing individual inputs of a bigger input array to be processed
    independently.

    Equivalent to:
    tf.keras.layers.Lambda(lambda inp: tf.slice(inp, self.begin, self.size))

    Args:
        begin (list | tuple): The starting indices for each dim slice. (0-indexed)
        size (list | tuple): The size of each dim slice. (1-indexed)

    See https://www.tensorflow.org/api_docs/python/tf/keras/Layer for more
    details.
    """
    def __init__(self, begin: list | tuple, size: list | tuple, **kwargs):
        super().__init__(**kwargs)
        self.begin = begin
        self.size = size

    def get_config(self) -> dict:
        """Return the layer's configuration.

        Saves layer configuration for saving, loading, and reconstruction.

        Returns:
            config (dict): A dictionary representing the layer's configuration.
        """
        config = super().get_config()
        config.update({
            'begin': self.begin,
            'size': self.size
        })
        return config

    def call(self, inputs: tf.Tensor) -> tf.Tensor:
        """Slice the input tensor.

        Args:
            inputs (tf.Tensor): The tensor to be sliced

        Returns:
            tf.Tensor: The sliced input tensor.
        """
        return tf.slice(inputs, self.begin, self.size)


@keras.saving.register_keras_serializable()
class Model(tf.keras.Model):
    """Create a model to process states and predict optimal outputs.

    The model will either be an actor or critic, depending on the output layer.
    The architecture besides the last layer is identical between the models.

    Args:
        is_actor (bool): Whether to create an actor or critic model.

    Methods:
        _cards_embedding: Shared embedding for a card representation.
    """
    def __init__(self, *args, is_actor: bool, **kwargs):
        super().__init__(**kwargs)
        self.optimizer = tf.keras.optimizers.Adam(
            learning_rate=config.learning_rate
        )
        init = tf.keras.initializers.VarianceScaling(1/3, mode='fan_in', distribution='uniform')
        L2 = tf.keras.regularizers.L2

        # State input is size (batch, #, 24, 40)
        self.deck = Slice([0, 0, 0], [-1, 15, 40], name='slice_deck')
        self.hand = Slice([0, 15, 0], [-1, 5, 40], name='slice_hand')
        self.o_hand = Slice([0, 20, 0], [-1, 1, 3], name='slice_o_hand')
        self.numerics = Slice([0, 21, 0], [-1, 3, 1], name='slice_nums')

        # =-= Card-Feature Embedding =-=
        self.flatten = tf.keras.layers.Flatten()
        self.remove_ids = Slice([0, 0, 1], [-1, -1, -1], name='remove_ids')

        self.c_dense0 = tf.keras.layers.Dense(32, activation='relu', name='c_dense0', kernel_initializer=init)
        self.c_dense1 = tf.keras.layers.Dense(32, activation='relu', name='c_dense1', kernel_initializer=init)
        self.n_dense0 = tf.keras.layers.Dense(16, activation='relu', name='n_dense0', kernel_initializer=init)
        self.oh_dense0 = tf.keras.layers.Dense(16, activation='relu', name='oh_dense0', kernel_initializer=init)

        # =-= Main Layers =-=
        self.gap1d = tf.keras.layers.GlobalAveragePooling1D()
        self.concat = tf.keras.layers.Concatenate(axis=1)
        self.dropout = tf.keras.layers.Dropout(0.1)

        self.dense0 = tf.keras.layers.Dense(256, activation='relu', name='dense0', kernel_initializer=init, kernel_regularizer=L2(0.001))
        self.dense1 = tf.keras.layers.Dense(256, activation='relu', name='dense1', kernel_initializer=init, kernel_regularizer=L2(0.001))
        self.dense2 = tf.keras.layers.Dense(256, activation='relu', name='dense2', kernel_initializer=init, kernel_regularizer=L2(0.001))
        self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, stateful=True, kernel_initializer=init, kernel_regularizer=L2(0.01))
        # self.project = tf.keras.layers.Dense(128, activation='linear', use_bias=False, name='project')

        if is_actor:
            self.out = tf.keras.layers.Dense(32, activation="softmax", name='actor', kernel_initializer=init, kernel_regularizer=L2(0.1))
        else:
            self.out = tf.keras.layers.Dense(1, activation='linear', name='critic', kernel_initializer=init, kernel_regularizer=L2(0.01))

    def get_config(self) -> dict:
        """Return the model's configuration.

        Saves model configuration for saving, loading, and reconstruction.

        Returns:
            config (dict): A dictionary representing the model's configuration.
        """
        config = super().get_config()
        config.update({
            'is_actor': self.out.name == 'actor'
        })
        return config

    def call(self, states: tf.Tensor) -> tf.Tensor:
        """Feed the state through the model.

        Args:
            states (tf.Tensor): A batch of states to process.

        Returns:
            tf.Tensor: The resulting state-value estimation (critic),
                or policy distribution (actor).
        """
        deck = self.deck(states)
        deck = self._cards_embedding(deck)
        deck = self.gap1d(deck)

        hand = self.hand(states)
        hand = self._cards_embedding(hand)
        hand = self.flatten(hand)

        o_hand = self.o_hand(states)
        o_hand = self.flatten(o_hand)
        o_hand = self.oh_dense0(o_hand)

        numerics = self.numerics(states)
        numerics = self.flatten(numerics)
        numerics = self.n_dense0(numerics)

        bt = self.concat([deck, hand, o_hand, numerics])
        bt = self.dense0(bt)
        bt = self.dense1(bt)
        bt = self.dense2(bt)
        bt_lstm = tf.expand_dims(bt, axis=0)
        bt_lstm = self.lstm(bt_lstm)
        # bt_lstm = self.project(bt_lstm)
        bt_lstm = bt_lstm[0]
        bt = bt_lstm
        return self.out(bt)

    def _cards_embedding(self, cards: tf.Tensor) -> tf.Tensor:
        """Shared embedding for a card representation.

        Args:
            cards (tf.Tensor): A card representation.

        Returns:
            tf.Tensor: The embedded card.
        """
        cards = self.remove_ids(cards)
        cards = self.c_dense0(cards)
        cards = self.c_dense1(cards)
        return cards


# Define player and opponent decks
p_deck = [
    core.cards_by_name['Devourer'],
    core.cards_by_name['Zasp'],
    core.cards_by_name['Mothiva'],
    core.cards_by_name['Leafbug Archer'],
    core.cards_by_name['Leafbug Archer'],
    core.cards_by_name['Leafbug Archer'],
    core.cards_by_name['Leafbug Ninja'],
    core.cards_by_name['Leafbug Ninja'],
    core.cards_by_name['Leafbug Ninja'],
    core.cards_by_name['Leafbug Ninja'],
    core.cards_by_name['Leafbug Clubber'],
    core.cards_by_name['Leafbug Clubber'],
    core.cards_by_name['Numbnail'],
    core.cards_by_name['Numbnail'],
    core.cards_by_name['Venus\' Bud']
]
o_deck = [
    core.cards_by_name['Tidal Wyrm'],
    core.cards_by_name['Astotheles'],
    core.cards_by_name['Monsieur Scarlet'],
    core.cards_by_name['Thief'],
    core.cards_by_name['Thief'],
    core.cards_by_name['Thief'],
    core.cards_by_name['Bandit'],
    core.cards_by_name['Bandit'],
    core.cards_by_name['Bandit'],
    core.cards_by_name['Burglar'],
    core.cards_by_name['Burglar'],
    core.cards_by_name['Burglar'],
    core.cards_by_name['Ironnail'],
    core.cards_by_name['Ironnail'],
    core.cards_by_name['Venus\' Bud']
]


env = core.SpyCardsEnv(p_deck, o_deck)
agent = Agent()

# Try to load model data and continue from where it left off.
try:
    model_data = pd.read_csv('ppo_data.csv')
    initial_episode = model_data['Episode'].iloc[-1] + 1
except FileNotFoundError:
    model_data = pd.DataFrame()
    initial_episode = 0

# Predict once using the model to build the layers.
agent.actor(tf.convert_to_tensor([env.reset()], tf.float32))
agent.actor.summary()
print(f"α: {config.learning_rate}, N: {config.batch_size}, γ: {config.gamma}, c2: {config.c2}")

# Train the model
for e in range(initial_episode, initial_episode + config.episodes):
    state = env.reset()
    states, actions, log_probs, rewards = [], [], [], []
    done = False

    # Play one episode, reset LSTM between episodes
    agent.reset_lstm()
    while not done:
        action, log_prob = agent.act(state)
        next_state, reward, done = env.step(action)
        states.append(state)
        actions.append(action)
        log_probs.append(log_prob)
        rewards.append(reward)
        agent.buffer.append((state, action, reward, log_prob, done))

        state = next_state

    # Fit the models to the new episode data, and save data.
    actor_loss, critic_loss, advantages = None, None, None
    if (e + 1) % config.batch_size == 0:
        actor_loss, critic_loss, advantages = agent.fit()
        agent.buffer.clear()
        print(f"Chosen Actions: {actions}, Episode: {e}, Reward: {sum(rewards):.2f}, Actor Loss: {actor_loss:.4f}, Critic Loss: {critic_loss:.4f}")
    else:
        print(f"Chosen Actions: {actions}, Episode: {e}, Reward: {sum(rewards):.2f}")
    model_data = pd.concat([model_data, pd.DataFrame({
        'Episode': [e],
        'Actions': [actions],
        'Reward': [rewards],
        'Log Probs': [log_probs],
        'Actor Loss': [actor_loss],
        'Critic Loss': [critic_loss],
        'Advantages': [advantages],
        'Round Length': [len(states)]
    })])

    # Models and data are saved every 50 episodes.
    if (e + 1) % 50 == 0:
        agent.actor.save('actor.keras')
        agent.critic.save('critic.keras')
        model_data.to_pickle('ppo_data.pkl')
